    close all;
    garo = 1920 * 100/125;
    sero = 1080* 100/125;
    chang = 80* 100/125; 
    
    X_ts = logsout.get('X');    
    U_ts = logsout.get('u');
    h_ts = logsout.get('h');

    % 데이터 추출
    X_data = X_ts.Values.Data;
    X1     = X_data(1,:);   
    X2     = X_data(2,:);   
    X3     = X_data(3,:);   
    X4     = X_data(4,:);   
    time = X_ts.Values.Time;  

    U_data = u_ts.Values.Data;
    U1     = U_data(1,:);  
    U2     = U_data(2,:);   

    h_data = h_ts.Values.Data;
    h1     = h_data(1,:);  
    h2     = h_data(2,:);  
    h3     = h_data(3,:);  
    h4     = h_data(4,:);      

    X = [X1;X2;X3;X4];
    U = [U1;U2];

    input = [X;U];
    output = [h1;h2;h3;h4];


    output(1,:) = output(1,:) + 1e-8 * randn(1, size(output,2));
    output(2,:) = output(2,:) + 1e-8 * randn(1, size(output,2));





% Solve an Input-Output Fitting problem with a Neural Network
% Script generated by Neural Fitting app
% Created 05-Jun-2025 23:36:51
%
% This script assumes these variables are defined:
%
%   input - input data.
%   output - target data.
% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.

% trainFcn = 'trainlm';  % Levenberg-Marquardt backpropagation.
% 
% % Create a Fitting Network
% hiddenLayerSize = 10;
% net = fitnet(hiddenLayerSize,trainFcn);
% 
% % Setup Division of Data for Training, Validation, Testing
% net.divideParam.trainRatio = 70/100;
% net.divideParam.valRatio = 15/100;
% net.divideParam.testRatio = 15/100;
% % Train the Network
% [net,tr] = train(net,x,t);

% === 네트워크 생성 및 설정 ===
n_input = 6; n_hidden = 10; n_output = 4;

net = network;
net = init(net);
net.numInputs = 1;
net.numLayers = 2;

net.inputConnect(1,1) = 1;             % 입력 → 은닉층
net.layerConnect(2,1) = 1;             % 은닉층 → 출력층
net.outputConnect(2)  = 1;             % 출력층 → 출력
net.biasConnect = [0; 0];              % Bias 제거

% === 정규화  ===
net.inputs{1}.processFcns = {};
net.outputs{2}.processFcns = {};

net.inputs{1}.size = n_input;
net.layers{1}.size = n_hidden;
net.layers{2}.size = n_output;

net.layers{1}.transferFcn = 'tansig';
net.layers{2}.transferFcn = 'purelin';

net.trainFcn = 'trainlm';
net.performFcn = 'mse';
net.divideFcn = 'dividerand';
net.divideParam.trainRatio = 0.7;
net.divideParam.valRatio   = 0.15;
net.divideParam.testRatio  = 0.15;
net.trainParam.min_grad = 1e-12;



% === 초기화 후, 수동 weight 지정 ===

net.IW{1,1} = randn(n_hidden, n_input) * 1e-2;
net.LW{2,1} = randn(n_output, n_hidden) * 1e-2;

% === 학습 ===
[net, tr] = train(net, input, output);


fprintf("📌 네트워크 구조 요약\n")
fprintf("──────────────────────────────\n")
fprintf("입력 정규화 함수     : %s\n", strjoin(net.input.processFcns, ", "))
fprintf("출력 정규화 함수     : %s\n", strjoin(net.output.processFcns, ", "))
fprintf("은닉층 활성함수      : %s\n", net.layers{1}.transferFcn)
fprintf("출력층 활성함수      : %s\n", net.layers{2}.transferFcn)
fprintf("사용된 학습 알고리즘 : %s\n", net.trainFcn)
fprintf("성능 평가 함수       : %s\n", net.performFcn)
fprintf("은닉 노드 수         : %d\n", net.layers{1}.size)
fprintf("입력 차원 수         : %d\n", net.inputs{1}.size)
fprintf("출력 차원 수         : %d\n", net.outputs{2}.size)
fprintf("──────────────────────────────\n\n")

% 📌 네트워크 구조 요약
% ──────────────────────────────
% 입력 정규화 함수     : mapminmax
% 출력 정규화 함수     : mapminmax
% 은닉층 활성함수      : tansig
% 출력층 활성함수      : purelin
% 사용된 학습 알고리즘 : trainlm
% 성능 평가 함수       : mse
% 은닉 노드 수         : 10
% 입력 차원 수         : 6
% 출력 차원 수         : 4
% ──────────────────────────────


% 저장 가능한 구조
V_hat = net.IW{1};     % [n_hidden × n_input]
W_hat = net.LW{2,1};   % [n_output × n_hidden]
b1 = net.b{1};         % [n_hidden × 1]
b2 = net.b{2};         % [n_output × 1]


% Plots
% Uncomment these lines to enable various plots.
%figure, plotperform(tr)
%figure, plottrainstate(tr)
%figure, ploterrhist(e)
%figure, plotregression(t,y)
%figure, plotfit(net,x,t)


